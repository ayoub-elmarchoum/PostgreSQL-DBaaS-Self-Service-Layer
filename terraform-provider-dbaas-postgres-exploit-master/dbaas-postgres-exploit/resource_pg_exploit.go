package dbaas

import (
	"context"
	"encoding/json"
	"fmt"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/resource"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"strings"
	"time"
)

// resourcePgExploit is a "factory function". Its sole purpose is to define and return the complete
// blueprint for the "dbaas-postgres-exploit_pg_exploit" resource. It doesn't execute any logic itself;
// instead, it constructs and provides the *schema.Resource object that tells Terraform everything
// it needs to know about this resource's structure, behavior, and lifecycle.
//
// The return type is `*schema.Resource`, which is a pointer to a schema.Resource struct.
// Using a pointer (`*`) is a standard Go practice for efficiency. It means we are passing around
// the memory address of this large blueprint object, rather than creating a full copy of it every time it's needed.
func resourcePgExploit() *schema.Resource {
	return &schema.Resource{
		// --- BEHAVIOR MAPPING (LIFECYCLE HOOKS) ---
		// This section maps the abstract Terraform lifecycle operations (Create, Read, Update, Delete)
		// to the concrete Go functions that we have written to implement that logic.
		// The "-Context" variants are the modern standard, as they accept a `context.Context`
		// for handling cancellations and deadlines gracefully.

		// When `terraform apply` is run on a new resource, Terraform will call this function.
		CreateContext: resourcePgExploitCreate,
		// When `terraform refresh` or `terraform apply` runs, this function is called to sync the state file with the remote resource.
		ReadContext: resourcePgExploitRead,
		// When `terraform destroy` is run, Terraform will call this function.
		DeleteContext: resourcePgExploitDelete,
		// When a change is detected on an existing resource during `terraform apply`, this function is called.
		UpdateContext: resourcePgExploitUpdate,
		// --- DATA SCHEMA DEFINITION ---
		// The `Schema` field is a map that defines the "API contract" for this resource as seen by the user in HCL files.
		// It lists every argument a user can configure and every attribute the provider will compute and return.
		Schema: map[string]*schema.Schema{
			// "tenant" is a required identifier for the resource.
			"tenant": {
				Type:        schema.TypeString,
				Required:    true,
				Description: "Tenant identifier",
			},
			// "action" specifies the operation to perform, e.g., "repack".
			"action": {
				Type:     schema.TypeString,
				Required: true,
				// `ValidateFunc` provides a way to enforce rules on the user's input before any other logic runs.
				// Here, it ensures the action is one of the known, supported values.
				ValidateFunc: validateActionType,
			},
			// "target_list" is a list of targets for the action.
			"target_list": {
				Type:     schema.TypeList, // Defines this argument as a list in HCL.
				Required: true,
				// `Elem` (Element) is crucial for lists. It defines the schema for *each object* inside the list.
				// We use `&schema.Resource` here to define the structure of these nested objects.
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"dbname": {
							Type:     schema.TypeString,
							Required: true,
						},
						"schema": {
							Type:     schema.TypeString,
							Optional: true,
						},
						"table": {
							Type:     schema.TypeString,
							Optional: true,
						},
						"no_kill_backend": {
							Type:     schema.TypeBool,
							Optional: true,
						},
						"timeout": {
							Type:     schema.TypeInt,
							Optional: true,
							Default:  3600,
						},
						"token_admin": {
							Type: schema.TypeString,
							//	Required: true,
							Optional: true,
							//	Default: "mandatoryforv1",
						},
					},
				},
			},
			// "metadata" contains job-related settings. It's a list with MaxItems: 1
			// to mimic a nested object structure in HCL.
			"metadata": {
				Type:     schema.TypeList,
				Required: true,
				// `MaxItems: 1` is the key pattern here. It tells Terraform to treat this as a single object block
				// rather than a list of multiple blocks. This is the standard way to model required nested objects in SDKv2.
				MaxItems: 1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Type:     schema.TypeString,
							Required: true,
						},
						"type": {
							Type:     schema.TypeString,
							Optional: true,
							Default:  getMetadata("type", nil).(string),
						},
						"affinity": {
							Type:     schema.TypeString,
							Optional: true,
							Default:  getMetadata("affinity", nil).(string),
						},
						"retry": {
							Type:     schema.TypeBool,
							Optional: true,
							Default:  getMetadata("retry", nil).(bool),
						},
						"wait_retry": {
							Type:         schema.TypeInt,
							Optional:     true,
							Default:      getMetadata("wait_retry", nil).(int),
							ValidateFunc: validation.IntAtLeast(15),
						},
						"timeout": {
							Type:         schema.TypeInt,
							Optional:     true,
							Default:      getMetadata("timeout", nil).(int),
							ValidateFunc: validation.IntAtLeast(15),
						},
					},
				},
			},
			// "output" is a computed attribute, meaning its value is generated by the provider,
			// not set by the user. It will contain the results of the job.
			"output": {
				Type: schema.TypeList,
				// `Computed: true` is the most important property here. It tells Terraform that this attribute
				// is *read-only* for the user. Its value is not configured in HCL but is instead "computed" or
				// populated by the provider, typically after an API call in the Create or Read functions.
				Computed: true,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						// These nested fields are also marked as computed, as they will be populated by the provider.
						"worker": {Type: schema.TypeString, Computed: true},
						"data":   {Type: schema.TypeString, Computed: true},
					},
				},
			},
		},
	}
}

// validateActionType is a validation function to ensure the "action" argument
// is one of the allowed values.
func validateActionType(val interface{}, key string) ([]string, []error) {
	v := val.(string)
	allowed := []string{"repack"}
	if !SliceFind(allowed, v) {
		return nil, []error{fmt.Errorf("%s must be one of %v", key, allowed)}
	}
	return nil, nil
}

// resourcePgExploitCreate handles the creation of a new resource.
// This function is executed by Terraform Core when a user runs `terraform apply` on a resource
// that does not yet exist in the Terraform state. Its goal is to create the resource on the
// remote system according to the user's configuration.
func resourcePgExploitCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// --- Step 1: Initialization ---
	// Retrieve the API client that was configured in the provider's `ConfigureFunc`.
	// This client is our gateway for all communication with the remote API.
	client := meta.(*api_client)
	// Read the configuration values provided by the user in their HCL file.
	// `d.Get()` is the standard way to access these values from the resource data.
	name := d.Get("metadata.0.name").(string)
	jobType := d.Get("metadata.0.type").(string)
	action := d.Get("action").(string)

	// --- Step 2: Prepare the API Request ---
	// Construct the JSON payload that will be sent as the body of the POST request.
	// This payload represents the desired state of the resource to be created.
	payload := map[string]interface{}{
		"tenant":      d.Get("tenant").(string),
		"action":      action,
		"target_list": d.Get("target_list"),
	}
	// Marshal the Go map into a JSON byte slice. `json.Marshal` converts the structured data into a JSON string.
	dataBytes, err := json.Marshal(payload)
	if err != nil {
		// If marshaling fails, it's a developer error, and we must stop immediately.
		return diag.FromErr(err)
	}
	// The target API expects the payload to be nested within a "database" key.
	// We format the final request body string to match this specific API requirement.
	data := fmt.Sprintf(`{"database": %s}`, string(dataBytes))
	// Construct the specific API endpoint path for creating this type of job.
	path := fmt.Sprintf("/jobs/%s/%s", jobType, name)
	// Prepare the necessary HTTP headers. While the client handles Authorization automatically,
	// setting "Content-Type" is best practice for POST/PUT requests with a JSON body.
	headers := map[string]string{
		"Content-Type":  "application/json",
		"Authorization": "Bearer " + client.token,
	}

	// --- Step 3: Trigger the Creation on the Remote API ---
	// Send the POST request. This is the action that initiates the creation process.
	// Since the API is asynchronous, this call likely returns immediately after starting a background job.
	jobraw, err := client.send_request("POST", path, data, headers)
	// Perform a basic check on the immediate HTTP response. `handleHTTPError` can catch
	// initial client-side errors (e.g., a 400 Bad Request) before we start waiting.
	baseMsg := fmt.Sprintf("Cannot do repack '%s' -", name)
	fullurl := fmt.Sprintf("%s%s", client.uri, path)
	err = handleHTTPError(err, jobraw, fullurl, baseMsg)
	if err != nil {
		// If the API immediately rejects the creation request, fail the Terraform operation.
		return diag.FromErr(err)
	}
	// --- Step 4: Wait for the Asynchronous Job to Complete ---
	// The resource is not considered "created" by Terraform until the remote job has finished successfully.
	// We call our dedicated polling function, `waitExploitJobState`, to handle this waiting period.
	// `isDeletion` is `false` because we are creating, not deleting, the resource.
	err = waitExploitJobState(ctx, "created", d, meta, false)
	if err != nil {
		// If the wait function returns an error (e.g., the job failed on the backend or timed out),
		// we must fail the Terraform creation operation.
		return diag.FromErr(err)
	}
	// --- Step 5: Finalize the Operation ---
	// Note: A common best practice is to call the Read function here (`return resourcePgExploitRead(ctx, d, meta)`).
	// This ensures that after the resource is successfully created, its final state (including any server-generated
	// values) is immediately read and saved into the Terraform state file. This current implementation
	// relies on the `waitExploitJobState` to have called Read, which is also a valid pattern.

	// Return an empty diagnostics object, which signifies a successful creation to Terraform.
	return diag.Diagnostics{}
}

// isZero is a helper function to check if a JobProperties struct is empty.
// This helps detect cases where the API returns an empty object instead of a 404.
func isZero(job JobProperties) bool {
	return job.Name == "" && job.Type == "" && job.State == ""
}

// waitExploitJobState is a crucial helper function designed to handle asynchronous operations.
// Many APIs start a job and return immediately. This function's purpose is to pause the Terraform
// operation and repeatedly check the job's status until it reaches a final state (like "finished" or "failed").
func waitExploitJobState(
	ctx context.Context, // The context for managing cancellation and deadlines.
	state string, // The *expected* intermediate state (e.g., "created", "updated"). Used for logging.
	d *schema.ResourceData, // The Terraform resource data, to read config and set the ID.
	meta interface{}, // The provider's configured metadata, which contains our API client.
	isDeletion bool, // A boolean flag to change the function's behavior when waiting for a deletion versus a creation/update.
) error {
	// Retrieve the API client that was configured in the provider's `ConfigureFunc`.
	// This client is our gateway for all communication with the remote API.
	client := meta.(*api_client)
	// Read all the necessary configuration settings from the resource data (`d`).
	// These settings control the behavior of our waiting logic.
	name := d.Get("metadata.0.name").(string)
	jobType := d.Get("metadata.0.type").(string)
	affinity := d.Get("metadata.0.affinity").(string) // Note: affinity is read but not used in the retry loop logic itself.
	retry := d.Get("metadata.0.retry").(bool)         // Should we attempt to re-trigger a job if it fails?
	timeout := d.Get("metadata.0.timeout").(int)      // The absolute maximum time in seconds to wait for the entire operation.
	waitRetry := d.Get("metadata.0.wait_retry").(int) // How long to wait before attempting a retry on a failed job.

	startTime := time.Now()
	// Use the Terraform SDK's built-in Retry helper. This is the standard way to handle polling.
	// It will execute the anonymous function repeatedly until the function returns `nil` (success)
	// or a `NonRetryableError` (failure). If the timeout is exceeded, it also returns an error.
	return resource.Retry(time.Duration(timeout)*time.Second, func() *resource.RetryError {
		// --- This anonymous function is the core of our polling logic ---

		// STEP 1: Always check the current status of the remote job.
		job, err := getJob(client, jobType, name)
		// STEP 2: Handle errors from the API call itself.
		if err != nil {
			// This is special logic for deletion. If we are deleting a resource, a "404 Not Found"
			// error is actually a SUCCESS state, because the resource is gone. We also treat some
			// 500 errors during deletion as success to be more robust, assuming the deletion will eventually proceed.
			if isDeletion &&
				(strings.Contains(err.Error(), "response code '404'") ||
					strings.Contains(err.Error(), "Job not found") ||
					strings.Contains(err.Error(), "Code HTTP inattendu : 500") ||
					strings.HasSuffix(err.Error(), "could not be deleted: Task error")) {
				// Return `nil` to signal success and stop the Retry loop.
				return nil
			}
			// For any other type of error, we stop immediately. It's not a temporary state.
			return resource.NonRetryableError(fmt.Errorf("Error when getting exploit job '%s': %s", name, err))
		}
		// STEP 3: Handle cases where the API returns a 200 OK but an empty object.
		if isZero(job) {
			if isDeletion {
				// If deleting and the job is empty, it's effectively gone. Success.
				return nil
			}
			// If creating/updating and the job is empty, something is wrong. Fail immediately.
			return resource.NonRetryableError(fmt.Errorf("Unexpected empty job returned for '%s'", name))
		}
		// STEP 4: Set the resource ID as soon as we confirm the job exists.
		// This tells Terraform that the resource is "real" and being tracked.
		d.SetId(fmt.Sprintf("%s/%s/%s", jobType, affinity, name))
		// STEP 5: Check for the terminal SUCCESS state.
		if job.State == "finished" || (isDeletion && (job.State == "deleted" || job.State == "dead")) {
			// Before we declare final success, we must run the Read function one last time.
			// This ensures all computed attributes (like `output`) are populated in the Terraform state.
			if err := PgexploitRead(ctx, d, meta); err != nil {
				return resource.NonRetryableError(err)
			}
			// Return `nil` to signal to `resource.Retry` that the operation has completed successfully.
			return nil
		}

		// STEP 6: Handle a specific edge case for deletion where a job might get stuck in "pending".
		if job.State == "pending" && isDeletion {
			// If we've already hit our main timeout, we give up and assume it's gone.
			if time.Since(startTime) >= time.Duration(timeout)*time.Second {
				return nil // Ignore pending state after maximum wait time
			}
			// Otherwise, we continue waiting.
			return resource.RetryableError(fmt.Errorf("Job '%s' is pending, waiting for state change", name))
		}
		// Build a generic error message for logging, in case we need to retry or fail.
		errorMsg := fmt.Errorf("Expected exploit job '%s' to be %s but was in state %s", name, state, job.State)
		// STEP 7: Check for a recoverable FAILURE state.
		if job.State == "failed" && retry {
			// If the job failed BUT the user enabled the `retry` option, we attempt to re-trigger it.
			time.Sleep(time.Duration(waitRetry) * time.Second) // Wait a bit before retrying.
			_, err := retryJob(client, jobType, name)
			if err != nil {
				// If the API call to *retry* the job fails, then it's a permanent failure.
				errorMsg = fmt.Errorf("%v%v", errorMsg, friendlyYAMLError(job))
				return resource.NonRetryableError(errorMsg)
			}
			// If the retry was triggered successfully, we will continue to the final `RetryableError` to keep polling.
		} else if job.State == "failed" || job.State == "dead" {
			// If the job failed and retry is NOT enabled, or the state is "dead", it's a permanent failure.
			errorMsg = fmt.Errorf("%v; details: %v", errorMsg, friendlyYAMLError(job))
			return resource.NonRetryableError(errorMsg)
		}
		// STEP 8: The DEFAULT case - the job is still in progress.
		// If none of the terminal conditions (success or failure) are met, the job is in a transient state
		// (e.g., "pending", "running"). We must signal to `resource.Retry` to wait and try this whole function again.
		errorMsg = fmt.Errorf("%v%v", errorMsg, friendlyYAMLError(job))
		return resource.RetryableError(errorMsg)
	})
}

// PgexploitRead contains the core "read" logic for the pg_exploit resource.
// Its primary role is to synchronize the Terraform state (what Terraform thinks exists)
// with the actual state of the resource on the remote system (the API).
// This function is called during `terraform refresh` and after a successful create or update operation.
func PgexploitRead(ctx context.Context, d *schema.ResourceData, meta interface{}) error {
	// Retrieve the API client that was configured in the provider's `ConfigureFunc`.
	// This client is our gateway for all communication with the remote API.
	client := meta.(*api_client)

	// --- Identifier Extraction Logic ---
	// The goal of this section is to reliably get the job's 'name' and 'jobType',
	// which are required to query the correct API endpoint.

	// The resource ID in the Terraform state is a simple string. We attempt to parse it
	// based on a known format (e.g., "type/affinity/name").
	idArr := strings.Split(d.Id(), "/")
	var name, jobType string
	// FIRST POSSIBLE CASE: The resource already exists in the state and has a well-formed ID.
	// This is the typical path for `terraform refresh` or an `apply` on an existing resource.
	// The ID is considered a reliable source of truth for identifying the resource.
	if len(idArr) >= 3 {
		jobType = idArr[0]
		name = idArr[2]
	} else {
		// SECOND POSSIBLE CASE: The resource ID is not set or is incomplete.
		// This often happens immediately after creation, where the Read function is called
		// before the ID has been finalized in the state.
		// In this scenario, we fall back to reading the identifiers from the user's configuration,
		// which is always available in the `d *schema.ResourceData` object.
		metadata := d.Get("metadata").([]interface{})[0].(map[string]interface{})
		name = metadata["name"].(string)
		jobType = metadata["type"].(string)
	}

	// Call our shared `getJob` helper function to query the API
	// and retrieve the current properties of the job.
	job, err := getJob(client, jobType, name)
	// CRITICAL ERROR HANDLING: A 404 "Not Found" status code is not a failure for a Read operation.
	// It simply means the resource was deleted on the remote system (perhaps manually) outside of Terraform's control.
	if err != nil {
		if strings.Contains(err.Error(), "response code '404'") {
			// We signal to Terraform that the resource no longer exists by clearing its ID in the state.
			// On the next `terraform plan`, Terraform will propose to re-create it.
			d.SetId("")
			// We return `nil` (no error) to indicate that the Read operation succeeded: it correctly
			// determined that the resource does not exist anymore.
			return nil
		}
		// For any other error (e.g., network issues, API server errors like 500), we return an error,
		// which will cause the Terraform operation to fail.
		return fmt.Errorf("Unable to retrieve exploit job '%s': %s", name, err)
	}

	// --- TERRAFORM STATE POPULATION ---
	// If we reach this point, the job was successfully found on the remote API.
	// The goal now is to update the Terraform state (`d`) with the values read from the API.

	// Set the "tenant" attribute in the Terraform state with the value returned by the API.
	if err := d.Set("tenant", job.Tenant); err != nil {
		return err
	}

	// Update the "metadata" block. This is a subtle but important section.
	// Instead of reading all metadata from the `job` object, we reconstruct it.
	// Why? To preserve the user's original settings (like `retry`, `timeout`) which might not be
	// returned by the API. This prevents Terraform from detecting an unnecessary "diff" if the
	// API doesn't echo back these specific configuration-only fields.
	metadata := []map[string]interface{}{{
		"name":     name,
		"affinity": "default", // This is hardcoded. In a more advanced implementation, it might come from the API (`job.Affinity`).
		"type":     jobType,
		// For these values, we read what's currently in Terraform's configuration (`d`) via the
		// `getMetadata` helper. This ensures stability in the state.
		"retry":      getMetadata("retry", d).(bool),
		"timeout":    getMetadata("timeout", d).(int),
		"wait_retry": getMetadata("wait_retry", d).(int),
	}}
	// Set the entire "metadata" block in the Terraform state.
	if err := d.Set("metadata", metadata); err != nil {
		return err
	}

	// Handle the "output" attribute, which is a computed result of the job.
	var jobOutput []JobOutput
	// We only populate this attribute if the job's state is "finished".
	// The `len(job.Output) > 0` check is a safeguard to prevent a panic if the `Output` slice is empty.
	// Note: This implementation only considers the *first* element of the job's output.
	// A more comprehensive version might loop through all outputs.
	if len(job.Output) > 0 && job.Output[0].State == "finished" {
		jobOutput = append(jobOutput, JobOutput{
			Worker: job.Output[0].Worker,
			Data:   job.Output[0].Data,
		})
	}
	// Set the "output" attribute in the state. It will be an empty list if the job is not finished.
	d.Set("output", jobOutput)
	// The read operation was successful. We return `nil` to signal this to Terraform.
	return nil
}

func resourcePgExploitRead(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	if err := PgexploitRead(ctx, d, meta); err != nil {
		return diag.FromErr(err)
	}
	return diag.Diagnostics{}
}

// resourcePgExploitDelete handles the destruction of the resource.
// This function is called by Terraform Core when a user runs `terraform destroy`
// or when a resource needs to be replaced during an update.
func resourcePgExploitDelete(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// Retrieve the API client that was configured in the provider's `ConfigureFunc`.
	// This client is our gateway for all communication with the remote API.
	client := meta.(*api_client)
	// Prepare the necessary HTTP headers for the request.
	// In this case, we only need to specify the content type, as the client
	// will automatically handle the Authorization token.
	headers := map[string]string{
		"Content-Type":  "application/json",
		"Authorization": "Bearer " + client.token,
	}

	// --- Step 1: Identify the resource to be deleted ---
	// Read the unique identifiers from the Terraform state (`d`) to know which resource to target.
	name := d.Get("metadata.0.name").(string)
	jobType := d.Get("metadata.0.type").(string)

	// Construct the specific API endpoint path for the target resource.
	path := fmt.Sprintf("/jobs/%s/%s", jobType, name)

	// --- Step 2: Trigger the deletion on the remote API ---
	// Send the DELETE request. This is the action that initiates the deletion process.
	// Note that the API is asynchronous, so this call only *starts* the job.
	_, err := client.send_request("DELETE", path, "", headers)
	if err != nil {
		// We check for a specific set of "ignorable" errors. For a deletion operation,
		// an error like "404 Not Found" is actually a success, because the resource is already gone.
		// We also leniently ignore some 500 server errors or task errors, assuming the deletion
		// might still succeed or is irrelevant if the resource is gone.
		if strings.Contains(err.Error(), "500") ||
			strings.Contains(err.Error(), "response code '404'") ||
			strings.Contains(err.Error(), "Job not found") ||
			strings.Contains(err.Error(), "Code HTTP inattendu : 500") ||
			strings.Contains(err.Error(), "could not be deleted: Task error") ||
			strings.HasSuffix(err.Error(), "could not be deleted: Task error") {
			// If the error is on our list of ignorable errors, we clear it and proceed.
			// This makes the destroy operation more resilient and less likely to fail on transient issues.
			err = nil // make in err "no error = nil"
		} else {
			// For any other, unexpected error, we must fail the operation immediately.
			return diag.FromErr(fmt.Errorf(
				"Cannot delete repack '%s' at '%s': %v",
				name,
				fmt.Sprintf("%s%s", client.uri, path),
				err))
		}
	}

	// --- Step 4: Wait for the deletion job to complete ---
	// The DELETE request was accepted, but now we must wait for the asynchronous job to finish.
	// We call our polling function, `waitExploitJobState`, for this.
	// The `isDeletion: true` flag is critical, as it tells the wait function to treat
	// "Not Found" errors as a success.
	err = waitExploitJobState(ctx, "deleted", d, meta, true)
	// --- Step 5: Handle the result of the waiting period ---
	if err != nil {
		// Similar to step 3, we apply idempotent logic to the result of the wait function.
		// If the wait function failed because the job disappeared (e.g., 404), that's our
		// desired outcome for a deletion, so we can treat it as a success.
		if strings.Contains(err.Error(), "response code '404'") ||
			strings.Contains(err.Error(), "Job not found") ||
			strings.Contains(err.Error(), "Code HTTP inattendu : 500") ||
			strings.Contains(err.Error(), "could not be deleted: Task error") ||
			strings.HasSuffix(err.Error(), "could not be deleted: Task error") {
			// The job is gone, which is what we want. Clear the error.
			err = nil
		} else {
			// If the wait failed for another reason (e.g., the job got stuck in a 'failed' state),
			// then we must report a failure for the destroy operation.
			return diag.FromErr(fmt.Errorf("Failed to wait for deletion of '%s': %v", name, err))
		}
	}

	// --- Step 6: Finalize the deletion in Terraform State ---
	// This is the most critical step for telling Terraform that the resource is gone.
	// Setting the ID to an empty string removes the resource from the state file.
	// Without this, Terraform would think the resource still exists and try to manage it in future plans.
	d.SetId("")
	// Return an empty diagnostics object, which signifies a successful operation to Terraform.
	return diag.Diagnostics{}
}

// resourcePgExploitUpdate handles changes to the resource's configuration.
// It's called by Terraform Core when `terraform apply` is run on a resource that already exists
// in the state but has one or more modified arguments in the HCL configuration.
func resourcePgExploitUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// This is the most critical optimization in an Update function.
	// The `d.HasChange()` function checks if any of the specified attributes have actually been modified
	// by the user since the last `apply`. The entire update logic is wrapped in this conditional.
	// If no relevant attributes have changed, the function does nothing and returns success,
	// avoiding unnecessary and potentially costly API calls.
	if d.HasChange("target_list") || d.HasChange("action") || d.HasChange("tenant") {
		// --- Inside this block, an actual update is required. ---

		// Retrieve the API client that was configured in the provider's `ConfigureFunc`.
		// This client is our gateway for all communication with the remote API.
		client := meta.(*api_client)

		// --- Step 1: Identify the resource to be updated ---
		// Read the unique identifiers from the Terraform state (`d`) to know which resource to target.
		// These identifiers are not expected to change during an update.
		name := d.Get("metadata.0.name").(string)
		jobType := d.Get("metadata.0.type").(string)

		// --- Step 2: Build the request payload for the PUT request ---
		// The payload includes all the configurable fields, reflecting the new desired state of the resource.
		// Note: This implementation sends the full object. Some APIs might support partial updates (HTTP PATCH)
		// with only the fields that have actually changed.
		payload := map[string]interface{}{
			"tenant":      d.Get("tenant").(string),
			"action":      d.Get("action").(string),
			"target_list": d.Get("target_list"),
		}
		// Marshal the Go map into a JSON byte slice.
		dataBytes, err := json.Marshal(payload)
		if err != nil {
			return diag.FromErr(err)
		}
		// The API expects the payload to be nested within a "database" key, so we format the final string accordingly.
		data := fmt.Sprintf(`{"database": %s}`, string(dataBytes))
		// Prepare the necessary HTTP headers for the request.
		headers := map[string]string{
			"Content-Type":  "application/json",
			"Authorization": "Bearer " + client.token,
			// The Authorization header is handled automatically by the client, but could be set here for an override.
		}

		// Construct the specific API endpoint path for the target resource.
		path := fmt.Sprintf("/jobs/%s/%s", jobType, name)

		// --- Step 3: Trigger the update on the remote API ---
		// Send the PUT request to the API. This is typically an asynchronous call that starts an update job
		// on the backend and returns immediately.
		jobraw, err := client.send_request("PUT", path, data, headers)
		// The `handleHTTPError` helper provides a first-level check on the immediate API response (e.g., 4xx client errors).
		baseMsg := fmt.Sprintf("Cannot update exploit '%s' -", name)
		fullurl := fmt.Sprintf("%s%s", client.uri, path)
		err = handleHTTPError(err, jobraw, fullurl, baseMsg)
		if err != nil {
			// If the API immediately rejects the request, we fail the Terraform operation.
			return diag.FromErr(err)
		}

		// --- Step 4: Wait for the asynchronous update job to complete ---
		// Just like in the Create function, the resource is not considered "updated" in Terraform
		// until the remote job has finished successfully.
		// We call our polling function, `waitExploitJobState`, to handle this.
		// `isDeletion` is set to `false` because this is an update, not a deletion.
		err = waitExploitJobState(ctx, "updated", d, meta, false)
		if err != nil {
			// If the wait function returns an error (e.g., the job failed or timed out),
			// we must fail the Terraform update operation.
			return diag.FromErr(err)
		}
	}
	// If the `if d.HasChange(...)` condition was false, the function jumps directly to here.
	// Returning an empty diagnostics object signifies a successful operation to Terraform.
	// In this case, it means "success, nothing needed to be done".
	return diag.Diagnostics{}
}
